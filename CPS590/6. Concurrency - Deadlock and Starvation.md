# <font style="color:#96DED1"> 6.1 - Principles of Deadlock</font>

---

- Deadlock can be defined as the _permanent_ blocking of a set of processes that either compete for system resources or communicate with each other
- A set of processes is deadlocked when each process in the set is blocked awaiting an even that can only be triggered by another blocked process in the set
- Deadlock is permanent because none of the events is ever triggered

![[Pasted image 20250401155129.png|300]]

- All 4 quadrants of the intersection are the resources over which control is needed
- Joint process diagram, illustrates the process of two processes competing for two resources
- Each process needs exclusive use of both resources fro a certain period of time
![[Pasted image 20250401155327.png|200]]
![[Pasted image 20250401155311.png|500]]

- x-axis represents progress in the execution of P
- y-axis represents progress in the execution of Q
- The joint progress of the two processes is represented by a path that progresses from the origin
- Vertical segments are when Q executes and P waits
- Horizontal segments are when P executes and Q waits
- Upward slanted lines are when both P and Q require A
- Downward slanted lines are when P and Q require B
- The slanted regions are forbidden

6 Paths of Execution
1) Q acquires B then A, then releases B and A. P resumes execution
2) Q acquires B then A, P executes and blocks on a request for A. Q releases B and A, P resumed execution
3) Q acquires B then P acquires A. Deadlock
4) P acquires A then Q acquires B. Deadlock
5) P acquires A then B, Q executes and blocks on a request for B. P releases A and B. Q resumes
6) P acquires A then B, then releases A and B. Q will resume execution

- The grey area is the **fatal region**, causing deadlock


![[Pasted image 20250401160203.png|200]]
![[Pasted image 20250401160219.png|500]]

- Regardless of the timing, deadlock cannot occur
- A high dimensional diagram is needed for more than two processes

# Reusable Resources
- Two categories of resources can be distinguished
	- Reusable
	- Consumable
- A reusable resource is one that can be safely used by only one process at a tome and is not depleted by that use
	- Processors
	- I/O channels
	- Main and secondary memory
	- Data structures (files, databases, semaphores)


![[Pasted image 20250402121432.png|500]]

- A strategy for dealing which such a deadlock is to impose system design constraints concerning the order in which resources can be requested
- Another example of deadlock with reusable resources has to do with requests for main memory
- Deadlock occurs if both processes progress to their second request
- The best way to deal with this problems, it to use virtual memory

![[Pasted image 20250402121706.png|300]]

# Consumable Resources
- A consumable resource is one that can be created and destroyed
- No limit on the number of consumable resources
- When a resource is acquired by a consuming process, the resource ceases to exist
	- Interrupts
	- Signals
	- Messages
	- Information in I/O buffers
- Deadlock occurs if the Receive is blocking

![[Pasted image 20250402122026.png|300]]

3 Strategies for Dealing with Deadlock:
1) Deadlock prevention
	1) Disallow one of the three necessary conditions for deadlock occurrence, or prevent circular wait condition from happening
2) Deadlock avoidance
	1) Do not grant a resource request if this allocation might lead to deadlock
3) Deadlock detection
	1) Grant resource requests when possible, but periodically check for the presence of deadlock and take action to recover

# Resource Allocation Graphs
- A useful tool in characterizing the allocation of resources to processes is the resource allocation graph
- Directed graph that depicts a state of the system of resources and processes, with each process and each resource represented by a node
- A graph edge directed form a process to resource indicates a resource that has been requested by the process but not yet granted
- Within a resource node, a dot is shown for each instance of that resource
- A graph edge directed from a reusable resource node dot to a process indicates a request that has been granted
- A graph edge directed from a consumable resource node dot to a process indicates the process is the producer of the resource
- (c) is an example of deadlock
- (d) there is no deadlock, since multiple units of each resource are available

![[Pasted image 20250403111144.png|400]]

- Below indicates the deadlock in (b), there is a circular chain of processes and resources that result in deadlock

![[Pasted image 20250403111324.png|400]]

# The Conditions for Deadlock
3 Conditions That Must be Present for Deadlock
1) Mutual exclusion
2) Hold and wait
3) No preemption

- Preemption must be supported by a rollback recovery mechanisms, which restores a process and its resources to a suitable previous state
- For deadlock to actually take place, a 4th condition is required 
	4) Circular wait

- The fourth condition is a potential consequence of the first three
- A fatal region exists only if all of the first three conditions listed above are met

Possibility of Deadlock:
1. Mutual exclusion
2. No preemption
3. Hold and wait

Existence of Deadlock:
1.  Mutual exclusion
2. No preemption
3. Hold and wait
4. Circular wait

- Prevent deadlock by adopting a policy that eliminates one of the conditions
- Avoid deadlock by making the appropriate dynamic choices based on the current state of resource allocation
- Detect the presence of deadlock and take action to recover

# <font style="color:#96DED1"> 6.2 - Deadlock Prevention</font>

---
- Constrain resource requests to prevent at least one of the four conditions of deadlock
- Deadlock prevention methods fall into two classes
	- An indirect method
		- Prevent the 3 conditions
	- A direct method
		- Prevent circular wait

# Mutual Exclusion
- Deadlock can occur if more than one process requires write permission

# Hold and Wait
- The hold and wait condition can be prevented by requiring that a process request all of its required resources at one time and blocking the process until al requests can be granted simultaneously
- Problems
	- A processes is held up for a long time
	- Resources allocated to a process may remain unused for a period of time
	- Process may not know in advance all of the resources that is will require
- An application would need to be aware of all resources that will be requested at all levels or in all modules to make the simultaneous request

# No Preemption
- If a process holding certain resources is denied a further request, that process must release its original resources, and request again
- If a process requests a resource that is held by another process, the OS may preempt the second process to requires it to release its resources

# Circular Wait
- Can be prevented by defining a linear ordering of resource types
- If a process has been allocated resources of type R, then is may request only those resources of type R in the ordering

# <font style="color:#96DED1"> 6.3 - Deadlock Avoidance</font>

---

- Allows the three necessary conditions but makes choices to assure that the deadlock point is never reaches
- This allows more concurrency than prevention
- Requires knowledge of future process resource requests

2 Approaches to Deadlock Avoidance:
1. Do not start a process if its demands might light to deadlock
2. Do not grant an incremental resource request to a process if this allocation might lead to deadlock

# Process Initiation Denial
- Consider a system $n$ processes and $m$ different types of resources

![[Pasted image 20250403112939.png|600]]

- The matrix Claim gives the maximum requirements of each process for each resource, with one row dedicated to each process
- The matrix Allocation gives the current allocation to each process

![[Pasted image 20250403113114.png|500]]

![[Pasted image 20250403113134.png|500]]

- A process is only started if the maximum claim of all current processes plus those of the new process can be met

# Resource Allocation Denial
- The strategy of resource allocation denial, referred to as the banker's algorithm
- The state of the system reflects the current allocation of resources to processes
- The state consists of two vectors
	- Resource
	- Available
- And two matrices
	- Claim
	- Allocation
- A safe state is one in which there is at least one sequence of resource allocations to processes that does not result in a deadlock
- An unsafe state is a state that is not safe

![[Pasted image 20250403114157.png|200]]

![[Pasted image 20250403114224.png|500]]

- Therefore, the initial state is safe by running processes, P2, P1, P3, P4
![[Pasted image 20250403115340.png|500]]

- `state, request [*]` is a vector defining the resources requested by process $i$
- Restrictions of deadlock avoidance:
	- The maximum resource requirement for each process must be stated in advance
	- The process under consideration must be independent; that is, the order in which they execute must be unconstrained by any synchronization requirements
	- There must be a fixed number of resources to allocate
	- No process may exit while holding resources


# <font style="color:#96DED1"> 6.4 - Deadlock Detection</font>

---
- Deadlock detection strategies do to limit resource access or restrict process actions

# Deadlock Detection Algorithm
- A check for deadlock can be made as frequently as each resource request, or less frequently
- Checking at each resource request has two advantages
	- Leads to early detection
	- Algorithm is relatively simple because it is based on incremental changes to the state of the system
- A request matrix is defined such that $Q_{ij}$ represents the amount of resources of type $j$ requested by process $i$
- The algorithm process
	- Mark each process that has a row in the Allocation matrix of all zeros
		- A process that has no allocated resources cannot participate in deadlock
	- Initialize a temporary vector W to equal the available vector
	- Find an index $i$ such that process $i$ is currently unmarked and the ith row of Q is less than or equal to W
		- If not row is found, terminate the algorithm
	- If row exists, mark process $i$ and add the corresponding row of the allocation matrix to W
	- Return to set 3

- A deadlock exists if and only if there are unmarked processes at the end of the algorhtm
- The set of unmarked rows corresponds to the set of deadlocked processes

![[Pasted image 20250403120241.png|400]]

1. Mark P4, since P4, has no allocated resource
2. Set W = (0 0 0 0 1)
3. The request of process P3 is less than or equal to W, so mark P3 and set

W = W + (0 0 0 1 0) = (0 0 0 1 1)
4. No other unmarked process has a row in Q that is less than or equal to W
	1. Therefore, terminate the algorithm

- The algorithm concludes with P1 and P2 unmarked, indicating these processes are deadlocked


# Recovery
- Strategies to recover from deadlock (increasing in sophistication):

	1. Abort all deadlocked processes
	2. Back up each deadlocked process to some previously defined checkpoint, and restart all processes
	3. Successively abort deadlocked processes until deadlock no longer exists
	4. Successively preempt resources until deadlock no longer exists

For 3 and 4, the selection criteria could be one of the following
1) Least amount of processor time consumed so far
2) Least amount of output produced so fat
3) Most estimated time remaining
4) Least total resources allocated so far
5) Lowest priority


# <font style="color:#96DED1"> 6.5 - An Integrated Deadlock Strategy</font>

---
- More efficient to use different strategies in different situations
	- Group resources into a number of different resource classes
	- Use a linear ordering strategy defined previously for the prevention of circular wait to prevent deadlocks between resource classes
	- Within a resource class, us the algorithm that is most appropriate for that class

Cases of Resources:
- Swappable spaces
	- Blocks of memory on secondary storage for use in swapping processes
- Process resource
	- Assignable devices, such as tape drives, and files
- Main memory
	- Assignable to processes in pages or segments
- Internal resources
	- I/O channels

Within Each Class the Following Strategy Can Be Used:
- Swappable space
	- Hold and wait prevention strategy
- Process resources
	- Avoidance, since expected processes are declared ahead of time
- Main memory
	- Prevention by preemption
	- When a process is preempted, it is simply swapped to secondary memory, freeing space to resolve the deadlock
- Internal resources
	- Prevention by means of resource ordering can be used


# <font style="color:#96DED1"> 6.6 - Dining Philosophers Problem</font>

---
- The dining philosophers problem, introduced by Dijkstra
- 5 philosophers live in a house, where a table is set for them
- Each require two forks
- Problem
	- Devise a algorithm that will allow the philosophers to eat
	- Must satisfy mutual exclusion, while avoiding deadlock and starvation

![[Pasted image 20250403122640.png|300]]

# Solution Using Semaphores
- Each philosopher first picks up the fork on the left then the fork on the right
- This leads to deadlock
- Buy 5 additional forks or teach to eat with one fork
- Consider adding an attendant that only allows four to eat at a time

![[Pasted image 20250403122915.png|500]]


# Solution Using a Monitor
- A vector of five condition variables is defined, one condition variable per fork
- Use to enable philosopher to wait for the availability of a fork
- Boolean vector that records the availability status of each fork (`true` means the fork is available)
- The monitor consists of two procedures
	- `get_fork`
	- `release-forks`
- Unlike the semaphore, solution, this monitor solution does not suffer from deadlock, because only one process at a time may be in the monitor
- The first philosopher process to enter the monitor is guarantees that is can pick up the right then left fork

![[Pasted image 20250403123217.png|500]]


# <font style="color:#96DED1"> 6.7 - UNIX Concurrency Mechanisms</font>

---

- UNIX provides a variety of mechanisms for interprocessor communication and synchronization
	- Pipes
	- Messages
	- Shared memory
	- Semaphores
	- Signals

- Pipes, messages, and shared memory can be used to communicate data between processes, whereas semaphores and signals are used to trigger actions by other processes

# Pipes
- Inspired by the concept of coroutines, a pipe is a circular buffer allowing two processes to communicate on the producer-consumer model
- FIFO queue, written by one process and read by another
- When a pipe is created it is given a fixed size in bytes
- When a process is written into a pipe, the write request is immediately executed if there is room, otherwise it is blocked
- This is similar to the reading process
- The OS enforces mutual exclusion, only one process can access a pipe at a time
- 2 types of pipes
	- Names
	- Unnamed
- Only related processes can share unnamed pipe, while either related or unrelated processes can share named pipes


# Messages
- A message is a block of bytes with an accompanying type
- UNIX provides `msgsnd` and `msgrcv` system calls for processes to engage in message passing
- Associated with each process is a message queue, which functions like a mailbox
- The message sender specifies the type of message
- The receiver can retrieve message in FIFO order or by type

# Shared Memory
- The fastest form of interprocess communication provided in UNIX is shared memory
- This is a common block of virtual memory shared by multiple processes
- Processes read and write shared memory using the same machine instructions
- Permission is read-only or read-write

# Semaphores
- The semaphores system call in UNIX System V are a generalization of the `semWait` and `semSignal` primitives
- The kernel does all the requested operations atomically
- Semaphore Elements
	- Current value of the semaphore
	- Process ID of the last process to operate on the semaphore
	- Number of processes waiting for the semaphore value to be greater than its current value
	- Number of processes waiting for the semaphore value to be zero
- Semaphores are typically created in sets
- `semctl` system call that allows all of the semaphore values in the set to be set at the same time
- `sem_op` takes a list of semaphore operations as an argument, each defined on one of the semaphores in a set

# Signals
- A signal is a software mechanisms that informs a process of the occurrence of asynchronous event
- Similar to a hardware interrupt, but does not employ priorities
- Processes can send each other signals, or the kernel may send signals internally
- Signals of a given type cannot be queued

![[Pasted image 20250403124926.png|600]]


# <font style="color:#96DED1"> 6.8 - Linux Kernel Concurrency Mechanisms</font>

---
- Linux includes all of the concurrency mechanisms found in other UNIX systems, such as SVR4, including pipes, messages, shared memory, and signals
- Linux also supports a special type of signalling known as real-time (RT) signals; part of the POSIX
- RT differ from UNIX Signals in 3 primary ways:
	- Signal delivery in priority is supported
	- Multiple signals can be queued
	- With standard signals, no value or message can be send to the target process, it is only a notification, with RT, it is possible to send a value along with the signal


# Atomic Operations
- Linux provides a set of operations that guarantee atomic operations on a variable
- These operations can be used to avoid simple race conditions
- An atomic operation executes without interruption and without interference
- On a uniprocessor system, a thread performing an atomic operation cannot be interrupted once the operation has started until the operation is finished
- On a multiprocessor system, the variable being operated on is locked from access by other threads until this operation is completed
- Two types of atomic operations are defined
	- Integer operations
	- Bitmap operations

![[Pasted image 20250403132550.png|600]]

- For atomic integer operations, a special data type is used, `atomic_t`
- The atomic integer operation can be used only on this data type, and not other operations are allowed on this data type
- Advantages
	- Atomic operations are never used on variables that might in some circumstances be unprotected from race conditions
	- Variables of this data type are protected from improper use by nonatomic operations
	- The compiler cannot erroneously optimize access to the value
	- This data type serves to high architecture-specific differences in its implementation
- A typical use of atomic integer data type is to implement counters
- Atomic bitmap operations operate on one of a sequence of bits at an arbitrary memory location indicated by a pointer variable
- Atomic operations are the simplest approach to kernel synchronization, and more complex locking mechanisms can be built on top of them

# Spinlocks
- The most common technique used for protecting a critical section in Linux is the spinlock
- Only one thread at a time can acquire a spinlock
- Any other thread attempting to acquire the same lock will keep trying until it can acquire the lock
- A spinlock is built on an integer location in memory that is checked by each thread before it enters its critical section
- If the value is 0, the threads sets the value to 1 and enters its critical section
- If the value is nonzero, the threads checks until value is 0

```c
spin_lock(&lock)
//Critical Section
spin_unlock(&lock)
```

_BASIC SPINLOCKS_
4 Types of Spinlocks
1) Plain
	1) Does not affect the interrupt state on the processor on which it is run
2) \_irq
	1) If interrupts are always enabled, then this spinlock should be used
3) \_irqsave
	1) If it is now known if interrupts are enabled, this is used
	2) When a lock is acquired, the current state of interrupts on the local processor is saved, to be restored when the lock is released
4) \_bh
	1) When an interrupt occurs, the minimum amount of work necessary is performed by the corresponding interrupt handler
	2) The _bottom half_, performs the remainder of the interrupt-related work

![[Pasted image 20250403133440.png|600]]

- Spinlocks are implemented differently on a uniprocessor vs a multiprocessor system
- For a uniprocessor
	- If kernel preemption is turned off, so a thread executing in kernel mode cannot be interrupted, then the locks are deleted at compile time
	- If kernel preemption is enabled, the spinlocks compile but are implemented as code that enables/disables interrupts
- On a multiprocessor
	- The spinlock is compiled into code that does in fact test the spinlock location


_READER-WRITER SPINLOCK_
- The reader-writer spinlock is a mechanism that allows a greater degree of concurrency within the kernel than the basic spinlock
- Allows multiple threads to have simultaneous access to the same data structure for reading only, but gives exclusive access to the spinlock for a thread that intends to update the data structure
- Each reader-writer spinlock consists of a 24-bit reader counter and an unlock flag

![[Pasted image 20250403133852.png|400]]

- Reader-writer spinlock favours readers over writers
- The spinlock cannot be preempted by a writer
- New readers may be added to the spinlock even while a writer is waiting


# Semaphores
- Linux provides a semaphore interface corresponding to that in UNIX SVR4
- Linux provides implementation of semaphores for its own used
- 3 types of semaphore facilities in the kernel
	- Binary semaphores
	- Counting semaphores
	- Reader-writer semaphores


_BINARY AND COUNTING SEMAPHORES_
- The function names `down` and `up` are used for the functions referred to as `semWait` and `semSignal`, respectively
- A counting semaphore is initialized using `sem_init`, which gives a names and assigned an initial value
- Binary semaphores, called MUTEXes in Linux, are initialized using `inti_mUTEX` and `init_MUTEX_LOCKED`, which initialized to 1 or 0
- 3 Version of `down (semWait)` Operations:
1. `down` test the semaphores and blocks if not available
	1. Thread will awaken when a corresponding `up` operation occurs
2. `down_interruptible` allows the thread to receive and respond to a kernel signal while being blocked on the down operation
	1. When woken by a signal, the function increments the count and returns `-ENTER`
	2. Used for device drivers and other service in which it is convenient to override semaphore operations
3. `down_trylock` makes is possible to try to acquire a semaphore without being blocked
4. If the semaphore is available, it is acquired

![[Pasted image 20250403134719.png|600]]

_READER-WRITER SEMAPHORES_
- Divides users into readers and writers
- Allows multiple concurrent readers (with no writers), but only a single writer
- Counting semaphore for readers, but a binary semaphore (MUTEX) for writers


# Barriers
- In some architectures, compilers and/or processor hardware may reorder memory access in source code to optimize performance
- Linux provides the memory barrier facility
- `rmb()` insures that no reads occur across the barrier defined by the place of the `rmb()`
- `wmb()` operation insures that no writes occur across the barrier defined by the place of `wmb()`
- `mb()` operation provides both a load and store barrier

Barrier Operations:
1. Barriers relate to machine instructions, namely loads and stores
	1. Higher-level languages involves both a load from location and a store
2. The `rmb, wmb, mb` dictate the behaviour of both the compiler and the processor

- The `barrier()` operation is a lighter-weigt version of `mb()`, in that is only controls the compiler's behaviour

![[Pasted image 20250403135238.png|500]]

- The `smp_rmb, smp_wmb, smp_mb` provide optimization for code that may be compiled on either a uniprocessor (UP) or symmetric multiprocessor (SMP)

_RCU (READ-COPY-UPDATE)_
- The RCU mechanism is an advanced lightweight synchronization mechanism which was integrated into the Linux kernel in 2002
- RCU uses in Linux kernel
	- Networking subsystem
	- Memory subsystem
	- Virtual file system
- RCU in other OS
	- DragonFly BSD
- As opposed to common Linux synchronization mechanisms, RCU readers are not locked
- RCU core API consists of 6 Methods
	- `rcu_read_lock()`
	- `rcu_read_unlock()`
	- `call_rcu()`
	- `synchronize_rcu()`
	- `rcu_assign_pointer()`
	- `rcu_dereference()`
- There are 20 minor RCU methods
- RCU provides access for multiple readers and writers to shared resource
- When a writer want to update that resource, it creates a copy of it, updates it, and assigns the pointer to point to a new copy
- The old version of the resource if freed
- Updating a pointer is an atomic operation
- Access to a shared resource by readers must be encapsulate within `rcu_read_lock()/rcu_read_unlock()` block
- Access to the pointer (ptr) of the shared resource within that block must be done by `rcu_dereference(ptr)`, and one should not invoke the `rcu_dereference()` method outside of such a block
- After a writer has created a copy and changed its value, the writer cannot free the old version until it is sure that all readers do not need it anymore
- This can be done by calling `synchronize_rcu()` or the nonblocking method `call_rcu()`
- `call_rcu()` references a callback, which will be invoked when the RCU mechanism is assured that the resource can be freed

# <font style="color:#96DED1"> 6.9 - Solaris Thread Synchronization Primitives</font>

---

- An addition to the concurrency of UNIX SVR4, Solaris supports 4 thread synchronization primitives:
1. Mutual exclusion (mutex) locks
2. Semaphores
3. Multiple readers, single writers locks
4. Condition variable

- Solaris implement these primitives within the kernel threads
- The initialization functions for the primitives fill in some of the data members
- Once a synchronization object is created, there are only two operations that can be performed
	- Enter (acquire lock)
	- Release (unlock)

![[Pasted image 20250403140553.png|500]]


# Mutual Exclusion Lock
- A mutex is used to ensure that only one thread at a time can access the resource protected by the mutex
- The thread that locks the mutex must be the one that unlocks it
- A thread attempts to acquire a mutex lock by executing the `mutex_enter`
- If it cannot set the lock, the blocking action depends on a type-specific information stored in the mutex object
- The default blocking policy is a spinlock
	- A blocked thread polls the status of the lock while executing in a busy waiting loop
	- An interrupt-based blocking mechanisms is optional

Operations on a Mutex Lock:
`mutex_enter()` - acquires the lock, potentially blocking if it is already held
`mutex_exit()` - Releases the lock, potentially unblocking a waiter
`mutex_tryenter()` - Acquired the locks if it is not already held

# Semaphores
Solaris Semaphores:
`sema_p()` - Decrements the semaphore, potentially blocking the thread
`sema_v()` - Increments the semaphore, potentially unblocking a waiting thread
`sema_tryp()` - Decrements the semaphore if blocking is not required

# Readers/Writer Lock
- Allows multiple threads to have simultaneous read-only access to an object protect by the lock
- Single thread to access the writing

`rw_enter()` - Attempts to acquire a lock as reader or writer
`rw_exit()` - Releases a lock as reader or writer
`rw_tryenter()` - Acquires the lock if blocking is not required
`rw_downgrade()` - A thread that has acquired a write lock converts it to a read lock
`rw_tryupgrade()` - Attempts to convert a reader lock into a writer lock

# Condition Variables
- A condition variable is used to wait until a particular condition is true
- Condition variable must be used to conjunction with a mutex lock
- This implements a monitor of the type

`cv_wait()` - Blocks until the condition is signaled
`cv_signal()` - Wakes up one of the threads blocks in `cv_wait()`
`cv_broadcast()` - Wakes up all of the thread blocked in `cv_wait()`

```c
mutex_enter(&m);
while (some_condition){
	cv_wait(&cv, &m);
}
mutex_exit(&m);
```




# <font style="color:#96DED1"> 6.10 - Windows Concurrency Mechanisms</font>

---
- Windows provides synchronization among threads as part of the object architecture
- The most important methods of synchronization are Executive dispatcher objects, user-mode critical sections, slim reader-writer locks, condition variables, and lock-free options


# Wait Functions
- The wait function allows a thread to block its own execution
- The wait functions do not return until the specified criteria have been met
- The type of wait function determines the set of criteria used
- If the criteria have not been met, the calling thread enters the wait state
- `WaitForSingleObject` function requires a handle to one synchronization object
- The function returns when the following occurs
	- The specified object is in the signaled state
	- The time-out interval elapses
		- Time-out interval can be set to `INFINITE` for no time out

# Dispatcher Objects
- The mechanism used by the Windows Executive to implement synchronization facilities is the family of dispatcher objects
- First 5 are designed to support synchronization
- The remaining have other uses
- Each dispatcher object instance can be in either a signaled or unsignaled state


![[Pasted image 20250403142249.png|600]]

- The event object is useful in sending a signal to a thread indicating that a particular event has occurred
- The mutex object is used to enforce mutually exclusive access to a resource, allowing only one thread object at a time to gain access (binary semaphore)
- Semaphore objects may be shared by threads in multiple processes
- Waitable timer object signals at a certain time and/or regular intervals

# Critical Sections
- Critical sections can be used only by the threads of a single process
- Process is responsible for allocating memory used by a critical section
- Declaring a variable `CRTICAL_SECTION`

# Slim Reader-Writer Locks and Condition Variables
- Windows Vista adds a user-mode reader-writer
- The reader-writer lock enters the kernel to block only after attempting to use a spinlock
- It is _slim_ in the sense that is normally only requires allocation of a single pointer-sized piece of memory
- SRW lock, declares a variable `InitializeSRWLock`
- Condition variables
	- Acquire exclusive lock
	- While (predicate() == FALSE) SleepCondition Variable()
	- Perform the protected operation
	- Release the lock

# Lock-free Synchronization
- Windows relies heavily on interlocked operations for synchronization
- Interlocked operations use hardware facilities to guarantee that memory locations can be read, modified, and written in a single atomic operation
	- `InterlockedIncrement`
	- `InterlockedCompareExchange`
- These so-called _lock-free_ synchronization primitives have the advantage that a thread can never be switched away from a processor while still holding a lock


# <font style="color:#96DED1"> 6.11 - Android Interprocess Communication</font>

---
- The Linux kernel includes a number of features that can be used for interprocess communication (IPC), including pipes, shared memory, messages, sockets, semaphores, and signals
- Android does not use these features for IPC, but rather adds to the kernel a new capability known as Binder
- Binder provides a lightweight remote procedure call (RPC) that is efficient in terms of both memory and processing requirements, and is well suited to the requirements of an embedded system
- The Binder is used to mediate all interaction between two processes
	- A component in one process (client) issues a call to the destination (service)
- The RPC mechanisms works between two processes on the same system, but running on different virtual machines
- The method for communicating with the Binder is the ioctl system call
- The ioctl call is a general-purpose system call for device-specific I/O operations
- The dashed vertical line represent threads in a process
- A process that hosts a service will spawn multiple threads so it can handle multiple requests concurrently

![[Pasted image 20250404115240.png|400]]

Interaction Process:
1. A client component, such as an activity, invokes a service in the form of a call with argument data
2. The call invokes a proxy, which is able to translate the call into a transaction with the Binder driver
	1. The proxy performs a procedure called marshalling, which converts higher-level applications data structures into a parcel
	2. The parcel is a container for a message that can be sent through the Binder driver
	3. Proxy then submits the transaction to the binder by a blocking ioctl call
3. Binder sends a signal to the target thread that wakes the thread up from its blocking ioctl call
4. The stub performs a procedure called unmarshalling, which reconstructs higher-level application data structures from parcels received through binder transactions
5. The called service component returns the appropriate result to the stub
6. The stub marshals the return data into a reply parcel then submits the reply parcel to the Binder via an ioctl
7. The Binder wakes up the calling ioctl in the client proxy, which gets the transaction reply data
8. The proxy unmarshals the result from the reply parcel and returns the result to the client component that issued the service call


# <font style="color:#96DED1"> 6.12 - Summary</font>

---

- Deadlock is the blocking of a set of processes that either compete for system resources or communicate with each other
- The blockage is permanent unless the OS takes some extraordinary action
- Deadlock may involve reusable or consumable resources
	- A reusable resource is one that is not depleted or destroyed
		- I/O channel
		- Region of memory
	- A consumable resource is one that is destroyed
		- Messages
		- Information in I/O buffers
- 3 general approaches to dealing with deadlock
	- Prevention
	- Detection
	- Avoidance



# <font style="color:#96DED1"> 6.12 - Key Terms, Review Questions, and Problems</font>

---
