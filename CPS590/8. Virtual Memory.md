# <font style="color:#96DED1"> 8.1 - Hardware and Control Structures</font>

---

- 2 Characteristics of paging and segmentation
	- All memory references within a process are logical addresses that are dynamically translated into physical addresses at run time
		- A process may be swapped out of main memory
	- A process may be broken up into a number of pieces (pages or segments) and those pieces need not be contiguously located in main memory during execution

![[Pasted image 20250404162014.png|600]]

- The portion of a process that is actually in main memory at any time is called the resident set
- As the process executes, things proceed smoothly as long as all memory references are to locations that are in the resident set
- If the processor encounters a logical address that is not in main memory, it generates an interrupt indicating a memory access fault
- The OS puts the interrupted process in a blocking state
- OS must bring into main memory the piece of the process that contains the logical address that caused the access fault
- OS issues a disk I/O (input/output) read request
- OS can dispatch another process to run while the disk I/O is performed
- Once desired piece is back in main memory, the process is back into a Ready state

Implications
1. More processes may be maintained in main memory
2. A process may be larger than all of main memory

- Because a process executes only in main memory, that memory is referred to as real memory
- Programmer perceives is as virtual memory

![[Pasted image 20250404162649.png|600]]

# Locality and Virtual Memory
- Only a few pieces of any given process are in memory, and therefore more processes can be maintained in memory
- Time is save since unused pieces are not swapped in and out of memory
- Thrashing is when the system spends most of its time swapping pieces rather than executing instruction
- Principle of locality states that program and data references within a process tend to cluster
- A few pieces of a process will be needed over a short period of time is valid
- For vrtual memory to be effective, there must be hardware support for the paging/segmentation and OS includes software managing the movement of pages/segments


# Paging
- The use of paging to achieve virtual memory was first used for the Atlas computer
- Simple paging, is when main memory is divided into a number of equal-size frames
- Each process is divided into a number of equal-size pages of the same length as frames
- A process is loaded by loading all of its pages
- With virtual memory paging, not all pages need to be loaded into main memory frame for execution

![[Pasted image 20250404163332.png|400]]

- Each process has its own page table
- Each page table entry (PTE) contains the frame number of the corresponding page in main memory
- Since only some of the pages of a process may be in main memory, a bit is needed in each page table entry to indicate whether the pages is present (P) in main memory or not
- The page table entry includes a modify (M) bit, indicating whether the contents of the page have been altered since the page wast last loaded into main memory

_PAGE TABLE STRUCTURE_
- The basic mechanisms for reading a word from memory involves the translation of a virtual, or logical address consisting of a page number and offset, into a physical address, consisting of a frame number and offset, using a page table
- When a process is running, a register holds the starting address of the page table for that process
- The page number of a virtual address is used to index that table and look up the frame number

![[Pasted image 20250404163835.png|500]]

- Most virtual memory schemes store page tables in virtual memory rather than real memory
- Page tables are subject to paging just like pages
- When a process is running, the page table must be in main memory
- Some processors use a two-level scheme to organize large page tables
- Each entry points to a page table
	- Pentium processor

![[Pasted image 20250404164057.png|400]]

- Byte-level addressing and 4-kb ($2^{12}$) pages, then the 4-GB ($2^{32}$) virtual address space is composed of $2^{20}$ pages


![[Pasted image 20250404164231.png|500]]

**

_INVERTED PAGE TABLE_
- A drawback of the type of page tables that were discussing is that their size is proportional to that of the virtual address space
- An alternative approach to the use of one or multiple-level page table is **inverted page table** structure
- Use on PowerPC, ultraSPARC, IA-64 architecture
- The page number portion of a virtual address is mapped into a hash value using a simple hashing function
- The hash value is a pointer to the inverted page table, which contains the page table entries
- There is one entry in the inverted page table for each real memory page frame, rather than one per virtual page
- A fixed proportion of real memory is required for the tables
- A chaining technique is used for managing the overflow
- The hashing technique results in chains that are typically short, between one and two entries
- The page table's structure is _inverted_ because it indexes page table entries by frame number rather than by virtual page number

![[Pasted image 20250404165414.png|500]]

- Each entry in the page table includes
	- Page number
		- Portion of the virtual address
	- Process identifier
		- Process that owns this page
	- Control bits
		- Flags, such as valid, referenced, and modified
	- Chain pointer
		- Field is null if there are no chained entries

![[Pasted image 20250404165835.png|500]]


_TRANSLATION LOOKASIDE BUFFER_
- Every virtual memory reference can cause two physical memory accesses
	- Fetch the page table entry
	- Fetch the desired data
- Most VM use high-speed cache for page table entires, called **translation lookaside buffer (TLB)**
- This cache functions in the same way as memory cache and contains those page table entries that have been most recently used
- If the bit is not set, then the page is not in main memory and a memory access fault called a **page fault**, is issued

![[Pasted image 20250404165855.png|400]]

- If the desired page is not in main memory, a page fault interrupt causes the page fault handling routine to be invoked
- Each entry in the TLB must include the page number as well as the complete page table entry
- This technique is referred to as associative mapping and is contrasted with the direct mapping, or indexing, used for lookup
- Virtual memory mechanisms must interact with the cache system
- A virtual address will generally be in the form of a page number, offset
- Once the real address is generated, which is in the form of a tag and a remainder, the cache is consulted to see if the block containing the word is present
- If so, it is returned to the CPU, if not it is retrieved from main memory

![[Pasted image 20250407103024.png|500]]

![[Pasted image 20250407103047.png|500]]

_PAGE SIZE_
- The smaller the page size, the lesser is the amount of internal fragmentation
- Although the smaller the page size, the greater number of pages required per process
- More pages per process mean larger page tables
- Secondary-memory devices, which are rotational, favour a larger page size for more efficient block transfer of data
- The pages in memory will all contain portions of the process near recent references
- The page fault rate should be low
- As the size of the page increases, each individual page will contain locations further and further from any particular recent reference
- The effect of the principle of locality is weakened and the page fault begins to rise
- The page fault rate will begins to fall as the size of a page approaches the size of the entire process (P)
- There are no page faults when a page is the entire process

![[Pasted image 20250407103605.png|500]]

- Page fault rate is also affects by the number of frames allocated to a processes
- For a fixed page size, the fault rate drops as the number of pages maintained in main memory grows
- A software policy interacts with a hardware design decision (page size)

![[Pasted image 20250407103730.png|300]]

- Contemporary programming techniques used in large programs tend to decrease the locality of references within a process
	- Object-oriented techniques encourage the use of many small program and data modules with references scattered over a relatively large number of objects over a short period of time
	- Multithreaded applications may result in abrupt changes in the instruction stream and in scattered memory references
- For a given size of TLB, as the memory size of processes grows and as locality decreases, the hit ratios on TLB accesses declines
- TLB can become a performance bottleneck
- Several microprocessor architectures support multiple page sizes, including MIPS R4000, Alpha, UltraSPARC x86, and IA-64
- Multiple page sizes provides the flexibility needed to use a TLB effectively
- Large contiguous regions in the address space of a process, such a program instructions, may be mapped using a small number of large pages, while thread stacks may be mapped using small page size
- Most commercial OS still support only one page size


# Segmentation
_VIRTUAL MEMORY IMPLICATIONS_
- Segmentation allows the programmer to view memory as consisting of multiple address space of segments
- Segments may be of unequal, dynamic size
- Memory references consist of a (segment number, offset) form of address

- Advantages of a segmented address space
	- Simplifies the handling of growing data structures
	- Allows programs to be altered and recompiled independently, without requiring the entire set of programs to be relinked and reloaded
	- Lends itself to sharing among processes
	- Lends itself to protection


_ORGANIZATION_
- Each process has its own segment table, and when all of its segments are loaded into main memory, the segment table for a process is created and loaded into main memory
- Each segment table entry contains the starting address of the corresponding segment in main memory, as well as the length of the segment
- Since only some of the segments of a process may be in main memory, a bit is needed in each segment table entry to indicate whether the corresponding segment is present in main memory or not
- Another control bit in the segmentation table entry is a modify bit, indicating whether the contents of the segment have been altered since the segment was last loaded into main memory
- If protection or sharing is manages at the segment level, then bits for that purpose will be required
- The basic mechanism for reading a work from memory involved the translation of a virtual address, or logical, address, consisting of segment number and offset, into a physical address, using a segment table
- When a particular process is running, a register holds the starting address of the segment table for that process
- The segment number of a virtual address is used to index that table and look up the corresponding main memory address for the start of the segment
- This is added to the offset portion of the virtual address to produce the desired read address

![[Pasted image 20250407105131.png|500]]


# Combined Paging and Segmentation
- Both paging and segmentation have their strengths
- Paging, which is transparent to the programmer, eliminates external fragmentation
- Since the pieces are swapped from main memory in fixed, equal size, there are sophisticated memory management algorithms
- Segmentation, which is visible to the programmer, allow growing data structures, modularity, and support for sharing and protection
- In a combined paging/segmentation system, a user's address space is broken up into a number of segments
- Each segment is turned into a number of fixed-size pages, which are equal in length
- If a segment has less than that of a page, the segment occupies just one page
- From the programmer's POV, a logical address still consists of a segment number and a segment offset
- From the system's POV, the segment offset is viewed as a page number and page offset for a page within the specified segment

![[Pasted image 20250407105620.png|500]]


# Protection and Sharing
- Segmentation lends itself to the implementation of protection and sharing policies
- Each segment table entry includes a length as well as a base address, a program cannot inadvertently access a main memory location beyond the limits of a segment
- To achieve sharing, it is possible for a segment to be referenced in the segment tables of more than one processes
- This is also true of paging systems
- Page structure of programs and data is not visible to the programmer

![[Pasted image 20250407110100.png|300]]

- A common scheme is to use a ring-protection structure
- Lower-number, or inner, rings enjoy greater privilege than higher-numbered, or outer, rings
- 0 is reserved for kernel function
- Basic principles of the ring system
	- A program may access only data that reside on the same ring or a less-privileged ring
	- A program may call services residing on the same or a more privileged ring


# <font style="color:#96DED1"> 8.2 - Operating System Software</font>

---
- 3 Fundamental Areas of Memory Management
	- Whether to use virtual memory techniques
	- To use paging, segmentation, or both
	- Algorithms employed for various aspects of memory management

- The first two areas depend on the hardware platform available
- Earlier UNIX implementations did not provide VM because the processors on which the system ran did not support paging or segmentation
- With the exception of OS for some older PC, such as MS-DOS, and specialized systems, all important OS provide VM
- Pure segmentation systems are becoming increasingly rare
- When segmentation is combined with paging, most of the memory management issues confronting the OS designer are in the area of paging
- The third area are the domain of OS software

![[Pasted image 20250407110613.png|400]]

- In each case, the key issue is one of performance
- Minimize the rate at which fault occurs, because page faults cause considerable software overhead
- The overhead includes deciding which resident page or page to replace, and I/O of exchanging pages
- OS must schedule another process to run during the page I/O, causing a process switch
- The performance of a set of policies depended on main memory size, the relative speed of main and secondary memory, the size and number of processes competing for resources, and the execution behaviour of individual programs
- The execution depends on the nature of the application, the programming language and compiler, and style of the code

# Fetch Policy
- The fetch policy determines when a page should be brought into main memory
- 2 common ways to fetch
	- Demand paging
	- Prepaging
- Demand paging, a page is brought into main memory only when a reference is made to a location on that page
- Prepaging, pages other than the one demanded by a page fault are brought in
- Prepaging exploits the characteristics of most secondary memory devices, such as dicks, which have seek times and rotational latency
- If the page of a process are stored contiguously in secondary memory, then it is more efficient to bring in a number of contiguous pages at one time rather than bringing them in one at a time over an extended period
- When a process is swapped out of memory and put in a suspended state, all of its resident pages are moved out
- When the process is resumed, all of the pages that were in main memory are returned


# Placement Policy
- The placement policy determines where in read memory a process is to reside
- In a pure segmentation system, the placement policy is an important design issue
	- Best-bit
	- First-bit
- A system that uses paging and segmentation, placement is irrelevant because the address translation hardware and the main memory access hardware can performs their functions for any page-frame combination with equal efficiency
- Non-uniform memory access (NUMA) multiprocessor, the distributed, shared memory of the machine can be referenced by any processor on the machine, but the time for accessing varies with the distance between the processor and the memory module


# Replacement Policy
- Common Concepts
	- How many page frames are to allocated to each active process?
	- Whether the set of pages to be considered for replacement should be limited to those of the process that cause the page fault or encompass all the page frames in main memory
	- Among the set of pages considered, which particular page should be selected for replacement?
- The first two concepts are _resident set management_, and the last is _replacement policy_
- All of the policies have as their objective to remove the page that is least likely to be referenced in the future
- Because of the principle of locality, there is often a high correlation between recent referencing history and near-future patterns
- Policies try to predict future behaviour on the basis of past behaviour
- The more sophisticated the replacement policy, the greater the overhead


_FRAME LOCKING_
- Some of the frames in main memory may be locked
- When a frame is locked, the page currently stored in that frame may not be replaces
- Kernel, and key control structures, are held in locked frames
- In addition to I/O buffers and other time-critical areas
- Locking is achieved by associating a lock bit with each frame


_BASIC ALGORITHM_
- There are certain basic algorithms that are used for the selection of a page to replace
- Replacement algorithms
	- Optimal
	- Least recently used (LRU)
	- First-In First-Out (FIFO)
	- Clock
- The optimal policy selects for replacement that page for which the time to the next reference is the longest
- This policy results in the fewest number of page faults
- This is impossible to implement, since the OS must be perfect
- The page address stream of the optimal case is

`2 3 2 1 5 2 4 5 3 2 5 2`

- And produces three page faults

![[Pasted image 20250407112427.png|500]]

LRU
- Replaces the page in memory that has not been referenced for the longest time
- By the principle of locality, this should be the page least likely to be referenced
- LRU policy does nearly as well as the optimal
- This approach is difficult to implement
- Tag each page with the time of its last reference
- Overhead would be tremendous
- Alternatively, one could maintain a stack of page references
- In the example, there are four page faults

FIFO
- Treats the page frames allocated to a process as a circular buffer, and pages are removed in round-robin style
- All that is required is a pointer that circles through the pages frames of the processes
- This is the simplest page replacement policy to implement
- This results in 6 page faults

CLOCK
- The simplest form of clock policy requires the association of an additional bit with each frame, referred to as the use bit
- When a page is first loaded into a frame in memory, the use bit is set to 1
- When a page is referenced, its use bit is set to 1
- For the algorithm, the set of frames that are candidates for replacement is considered to be a circular buffer, which a pointer is associates
- When a page has a  bit of 0, it is replaced
- The clock goes around each frame with a user bit of 1, and it resets to 0, if a 0 is not fount
- This is simple to FIFO, except that, in the clock policy, any frame with a use bit of 1 is passed over by the algorithm
- The clock replacement policy results in 5 page faults

![[Pasted image 20250407113535.png|400]]

- For small allocations, with FIFO being over a factor of 2 worse than optimal
- All four curves have the same shape as the idealized behaviour
- The clock algorithm has been compared to these other algorithms when a variable allocation and either global or local replacement scope is used
- The clock algorithm was found to approximately closely perform of LRU
- The clock algorithm is made more powerful by increasing the number of bits that it employs

![[Pasted image 20250407114029.png|400]]

- Page replacement algorithm cycles through all of the pages in the buffer, looking for one that has not been modified since being brought in and has not been accessed recently
- This strategy was used on an earlier version of Mac VM
- The advantage of this algorithm over the simple clock algorithm is that pages that are unchanged are given preference for replacement


_PAGE BUFFERING_
- Although LRU and clock policies are superior to FIFO, they both involved complexity and overhead not suffered with FIFO
- The page replacement algorithm is simple FIFO
- To improve performance, a replaced page is not lost but rather assigned to one of 2 lists (page buffering)
	- The free page list if the page has not been modified
	- The modified page list if it has
- The page is not physically moved about in main memory, instead, the entry in the page table for this page is removed and placed in either the free or modified page list
- The free page list is a list of page frames available for reading in pages
- VMS tries to keep some small number of frames free at all times
- When an unmodified page is to be replaced, it remained in memory and its page frame is added to the tail of the free page list
- If the process references that page, it is returned to the resident set of the process at little cost
- Free and modified page lists acts as a cache of pages
- The modified page list are written out in clusters rather than one at a time, this reduces I/O operations and disk access time
- A simple version of page buffering is implemented in the Mach OS


_REPLACEMENT POLICY AND CACHE SIZE_
- Main memory size is getting larger and the locality of applications is decreasing
- In compensation, cache size have been increasing
- Large cache sizes, are not feasible design alternatives
- With large cache sizes, the replacement of VM pages can have a performance impact
- If the page frame selected for replacement is in the cache, then that cache block is lost as well the page that is holds
- A careful page placement strategy can result in a 10-20% fewer cache misses than naive placement


# Resident Set Management
- The portion of a process that is actually in main memory at any time is defined to be the resident set of the process


_RESIDENT SET SIZE_
- With pages VM, it is not necessary to bring all of the pages of a process into main memory
- OS must decide how many pages to bring, and how much main memory to allocate to each processes

Factors
1) The smaller the amount of memory allocated, the more processes that can reside in main memory, reducing the time lost due to swapping
2) If a relatively small number of pages of a process are in main memory, then, despite the principle of locality, the rate of page faults will be rather high
3) Beyond certain size, additional allocation of main memory to a particular process will have no noticeable effect on the page fault rate for that process because of the principle of locality

- Two policies are to be found in contemporary OS
	- Fixed allocation
		- Gives a process a fixed number of frames in main memory within which to execute
		- Decided at initial load time (process creation time) and may be determined based on the type of process
		- When a page fault occurs, one of the pages of that process must be replaced by the needed page
	- Variable allocation
		- Allows the number of page frames allocated to a process to be varied over the lifetime of the processes
		- A process that has persistently high level of page faults, will be given additional page frames to reduce the page fault rate
		- A process that has a low page fault rate, will be given a reduction allocation
		- More powerful policy, although more difficult to implement


_REPLACEMENT SCOPE_
- The scope of replacement strategy can be categorized as global or local
- Both are activated by a page fault when there are not free page frames
- A local replacement policy chooses only among the resident pages of the process that generated the page fault in selecting a page to replace
- A global replacement policy considers all unlocked pages un main memory, regardless of which process owns a particular page
- There is a correlation between replacement scope and resident set size
- A fixed resident set implies a local replacement policy
	- To hold the size of a resident set fixed, a page that is removed from main memory must be replaced by another page from the same praocess
- A variable-allocation policy can clearly employ a global replacement policy
	- The replacement of a page from one process in main memory with that of another causes the allocation of one process to grow by one page, and that of the other to shrink by one page

![[Pasted image 20250407124409.png|500]]


_FIXED ALLOCATION, LOCAL SCOPE_
- A process is running in main memory with a fixed number of frames
- When a page fault occurs, the OS must choose which page it to be replace from the currently resident pages for this processes
- It is necessary to decide ahead of time the amount of allocation to give to a processes
- Disadvantages
	- If allocations tend to be too small, then there will be a high page fault rate
	- If allocation are too large, there will be too few programs in main memory

_VARIABLE ALLOCATION, GLOBAL SCOPE_
- Easiest to implement and adopted in a number of OS
- OS also maintains a list of free frames
- When a page fault occurs, a free frame is added to the resident set of a process, and the page is brought in
- A process experiencing page fault will gradually grow in size, which reduced overall page faults in the system
- When there are no free frames available, the OS must choose a page currently in memory to replace
- The page selected can belong to any of the resident processes
- The process that suffers the reduction in resident set size may not be optimum
- Using page buffering, the choice of which page to replace becomes less significant

_VARIABLE ALLOCATION, LOCAL SCOPE_
- Attempts to overcome the problems with a global-scope strategy
	- When a new process is loaded into main memory, allocate it a certain number of page frames as its resident set, based on application type, program request, or other criteria
	- When a page fault occurs, select the page to replace from among the resident set of the process that suffers the fault
	- From time to time, reevaluate the allocation provided to the process and increase or decrease it to improve performance
- Working set strategy is a concept introduced and popularized by Denning
- Has a profound impact on VM management design
- A working set with parameter $∆$  for a process at virtual time $t$, which we designate as $W(t, ∆)$, is the set of pages of that process that have been referenced in the last $∆$ virtual time units

![[Pasted image 20250407125421.png|500]]

- Below indicates the way in which the working set size can vary over time for a fixed value of
- For many programs, period of relatively stable working set sizes alternate with periods of rapid change
- When a process first executes, it gradually builds up to a working set as it references new pages
- Eventually, by the principle of locality, the process should stabilize on a certain set of pages
- Transient periods reflect a shift of the program to a new locality

![[Pasted image 20250407125644.png|500]]

Resident Set Size Guide
1. Monitor the working set of each process
2. Periodically remove from the resident set of a process those pages that are not in its working set (LRU)
3. A process may execute only if its working set is in main memory

Disadvantages
1. The past does not always predict the future
2. A true measurement of working set of each process is impractical
	1. Necessary to time-stamp every page reference using the virtual time
3. The optimal value of $∆$ is unknown and in any case would vary

- The page fault rate falls as we increase the resident set size of a process
- The working set size should fall at a point on this curve such as indicated by W in the figure
- Rather than monitor the working set size directly, we can achieve comparable results by monitoring the page fault rate
- If the page fault rate of a process is below some minimum threshold, the system as whole can benefit by assigning smaller resident set sizes to this processes
- If the page fault is above, the process can benefit from an increased resident set size
- An algorithm that follows this strategy is the page fault frequency (PFF) algorithm
- The bit is set to 1 when the page is accessed
- When a page fault occurs, the OS notes the virtual time since the last page fault (maintaining a counter of page references)
- A threshold $F$ is defined, if the amount of time since the last page fault is less than $F$, then a page is added to the resident set
- Otherwise, discard all pages with a use bit of 0, and shrink the resident set
	- An upper threshold that is used to trigger a growth in the resident set size
	- A lower threshold used to contract the resident set size
- The time between page faults is the reciprocal of the page fault rate
- The major flaw in PFF, is that is does not perform well during the transient periods, when there is a shift to a new locality
- With PFF, no page every drops out of the resident set before $F$ virtual time units have elapsed of page faults causes the resident set of a process to swell before the pages of the old locality are expelled
- An approach to deal with inter-locality transition, with low overhead, is variable-interval sampled working set (VSWS) policy
- VSWS evaluates the working set of a process at sampling instances based on elapsed virtual time
- At the beginning, the use bits of all the resident pages for the process are reset, at the end, only the pages that have been referenced during the interval will have their bit set, and are retained in the resident set for the next interval
- Resident set size can only decrease at the end of an interval

M: minimum duration of the sampling interval
L: maximum duration of the sampling interval
Q: number of page faults that are allowed to occur between sampling instances

VSWS Policy
1. If the virtual time since the last sampling instance reaches L, then suspend the process and scan the use bits
2. If, prior to an elapsed virtual time of L, Q page faults occurs
	1. If the virtual time since the last sampling instance is less then M, then wait until the elapsed virtual time reaches M to suspend the process and scan for use bits
	2. If the virtual time is greater or equal to M, suspend the process and scan the use bits

- The VSWS policy tries to reduce the peak memory demands caused by abrupt inter-locality transitions by increasing the sampling frequency, and the rate at which unused pages drop out of the resident set, when page fault rate increases

# Cleaning Policy
- The cleaning policy is the opposite of a fetch policy; it is concerned with determining when a modified page should be written out to secondary memory
- 2 common alternatives
	- Demand cleaning
	- Pre-cleaning
- Demand cleaning, a page is written out to secondary memory only when it has been selected for replacement
- A pre-cleaning policy write modified pages before their page frames are needed so pages can be written out in batches
- With pre-cleaning, a page is written out but remains in main memory until the page replacement algorithm makes little sense to write out hundreds or thousands of pages only to find that the majority of them have been modified again before they are replaced
- Demand cleaning, the writing of a dirty page is coupled to, and precedes, the reading in of a new page
- A better approach incorporates page buffering
- This adopts the policy
	- Clean only pages that are replaceable
	- But decouple the cleaning and replacement operations
- With page buffering, replaced pages can be placed on 2 lists
	- Modified
	- Unmodified
- The pages on the modified list can periodically be written out in batches and moved to the unmodified list
- A page on the unmodified list is either reclaimed or lost when its frame is assigned to another page

# Load Control
- Load control is concerned with determining the number of processes that will be resident in main memory
- The load control policy is critical in effective memory management
- If two few processes, system will become blocks
- If two many processes, to much swapping will cause thrashing


_MULTIPROGRAMMING LEVEL_
- A point is reaches at which the average resident set is inadequate
- At this point, the number of page faults rises dramatically, and processor utilization collapses

![[Pasted image 20250408130347.png|300]]

- A working set or PFF algorithm implicitly incorporates load control
- Only those processes in resident set is sufficiently large are allowed to execute
- Another approach, suggests by Denning is known as the L = S criterion
- Adjusts the multiprogramming level so that mean time between faults equals the mean time required to process a page fault
- Performance studies indicates this is the point at which processor utilization attained a max
- Another approach is to adapt the clock page replacement algorithm
- This is a technique that uses global scope, that involved monitoring the rate at which the pointer scans the circular buffer of frames
	- Few page faults are occurring, resulting in few requests to advance the pointer
	- For each request, the average number of frames scanned by the pointer is small, indicating there are many resident pages not being referenced and are readily replaceable


_PROCESS SUSPENSION_
- If the degree of multiprogramming is to be reduced, one or more of the currently resident processes must be suspended (swapped out)

6 Possibilities:
1) Lowest-priority process
	1) Scheduling policy decision, and is unrelated to performance issues
2) Faulting processes
	1) There is a greater probability that the faulting task does not have its working set resident, and performance would suffer least by suspending it
3) Last process activates
	1) Least likely to have its working set resident
4) Process with the smallest resident set
	1) Require the least future effort to reload, penalized programs with strong locality
5) Largest processes
	1) Obtains the most free frames in an overcommitted memory
6) Process with the largest remaining execution window
	1) Run for a certain amount of time
	2) Shortest-processing-time-first scheduling discipline


# <font style="color:#96DED1"> 8.3 - UNIX and Solaris Memory Management</font>

---

- UNIX intended to be machine independent
- Earlier version of UNIX simply used variable partitioning with no VM scheme
- Current implementations of UNIX and Solaris use pages VM
- In SVR4 and Solaris, there are two separate memory management schemes
- The paging system provides VM that allocates page frames in main memory
- A kernel memory allocator is used to page VM for the kernel

# Paging System

_DATA STRUCTURES_
- For pages VM, UNIX makes use of data structures
	- Page table
		- One page table per process
		- One entry for each page in VM for that process
	- Disk block descriptor
		- Associated with each page of a process
	- Page frame data table
		- Describes each frame of real memory and is indexed by frame number
	- Swap-use table
		- One swap-use table for each swap device

![[Pasted image 20250408131636.png|400]]

![[Pasted image 20250408131705.png|400]]

- The Age field in the page table entry is an indication of how long it has been since a program referenced this frame
	- There is no universal UNIX use of this field for page replacement policy
- The Type of Storage field in the disk block descriptor is needed for:
	- An executable file is first used to create a new process
	- only a portion of the programmed data for that file may be loaded into real memory


_PAGE REPLACEMENT_
- The page frame data is used for page replacement
- Pointers are used to create lists within this table
- All of the available frames are linked in a list of free frames available for bringing in pages
- When the available frames drops below a threshold, a kernel will steal a number of frames to compensate
- The two-handed clock algorithm is used, and references the bit in the page table entry for each page in memory that is eligible to be swapped out

![[Pasted image 20250408132124.png|300]]

- The bit is set to 0 when the page is first brought in, and set to 1 when the page is referenced for read/write
- One hand in the algorithm, the front hands, sweeps through the pages on the list of eligible pages and sets the reference bit to 0 on each page
- Later the back hand sweeps through the same list and checks the reference bit
- If the bit is set to 1, the frames are ignores, otherwise they are swapped out

2 Parameters determine the Algorithm:
1. Scanrate
	1. Rate at which the two hands scan through the page list, in pages per second
2. Handspread
	1. The gap between the front hand and back hand

- These parameters default at boot time
- The scanrate parameter can be altered to meet changing conditions
- The parameter varies linearly between the values slowscan and fastscan as the amount of free memory varies between the values _lotsfree_ and _minfree_
- As the amount of free memory shrinks, the clock hands move more rapidly to free up more pages
- Together scanrate, and handspread determines the window of opportunity to use a page before it is swapped out due to lack of use

# Kernel Memory Allocator
- The kernel generates and destroys small tables and buffers during execution

Dynamic Memory Allocation:
1) The pathname translation routing may allocate a buffer to copy a pathname from user space
2) The `allocb()` routine allocates STREAMS buffers of arbitrary size
3) Many UNIX implementations allocate zombie structures to retain exit status and resource usage information about deceased processes
4) In SVR4 and Solaris, the kernel allocates many objects dynamically when needed

- In SVR4, a modification of the buddy system is used
- The cost to allocate and free a block of memory is low compared to that of best-fit or first-fit policies
- In the case of kernel, allocation must be as fast as possible
- The drawback of the buddy system is the time required to fragment and coalesce blocks
- To avoid unnecessary coalescing and splitting, the lazy buddy system defers coalescing until it seems likely that it is needed

Lazy Buddy System Parameters:

![[Pasted image 20250408133146.png|500]]

- In general, the lazy buddy system tries to maintain a pool of locally free blocks and only invoked coalescing if the number of locally free blocks exceeds a threshold

![[Pasted image 20250408133257.png|500]]



# <font style="color:#96DED1"> 8.4 - Linux Memory Management</font>

---
- Linux memory management scheme is complex
	- Process VM
	- Kernel memory allocation
- The basic unit of memory is a physical page, which is represented in Linux kernel by struct page
- The size of the page depended on architecture, typically 4kB
- Linux also supports Hugepages, for page sizes of 2MB


# Linux Virtual Memory

![[Pasted image 20250408133610.png|500]]


_VIRTUAL MEMORY ADDRESSING_
- Linux uses a 3-level page table structure, consisting of the types of tables (each table is the size of one page)
1) Page directory
	1) A process has a single page directory
	2) Each entry points to one page of the page middle directory
	3) Must be in main memory
2) Page middle directory
	1) Spans multiple pages
	2) Each entry points to one page in the page table
3) Page table
	1) May also span multiple pages
	2) Each entry refers to one virtual page of the process

- The virtual address in Linux is viewed as 4 fields
- The last field gives the offset within the selected page of memory
- The Linux page table is designed to accommodate the 64-bit Alpha processor


_PAGE ALLOCATION_
- The kernel maintains a list of contiguous page frame groups of fixed size, such as 1, 2, 4, 8, 16, 32 page frames
- As pages are allocated and deallocated in main memory, the available groups are split and merged using the buddy system

_PAGE REPLACEMENT ALGORITHM_
- Earlier versions of Linux was based on the clock algorithm
- In the Linux scheme, the use bit was replaced with an 8-bit age variable
- Each time that a page is accessed, the age variable is incremented
- Linux periodically sweeps through the global page pol and decrements the age variable
- A page with "0" is "old"
- The larger the value of age, the more frequently a page has been used
- Linux algorithm was a form of least frequently used policy
- New Linux, replaces the page replacement algorithm with a split LRU algorithm, that was merged into the kernel
- New algorithm uses 2 flags to each page table entry
	- PG_active
	- PG_referenced
- A kernel daemon `kswapd` runs to periodically page reclaim in each zone

1. The first time a page on the inactive list is accessed, the PG_referenced flag is set
2. The next time, it is moved to the active list
	1. Takes 2 accesses for a page to be declared active
3. If a second access does not happen, PG_referenced is reset
4. For active page, two timeouts are required to move the page to the inactive list

![[Pasted image 20250408134723.png|400]]


# Kernel Memory Allocation
- The Linux kernel memory capability manages physical main memory page frames
- Possible owners of a frame include user-space processes, dynamically allocated kernel data, static kernel code, and the page cache
- A buddy algorithm is used so memory for the kernel can be allocated and deallocated un units of one of more pages
- Linux uses _slab allocation_ within an allocated page
- On a x86 machine, the page size is 4MB, and chunks within a page are sizes:
	- 32, 64, 128, 252, 508, 2050, and 4080 bytes
- The SLAB allocator is complex; maintains a set of linked lists, one for each size of chunk

3 Types of SLAB Allocators
1) SLAB
	1) Designed to be as cache-friendly as possible, minimizing cache misses
2) SLUB (unqueued slab allocator)
	1) Designed to be simple and minimize instruction count
3) SLOB (simple list of blocks)
	2) Designed to be as compact as possible, intended for systems with memory limitations


# <font style="color:#96DED1"> 8.5 - Windows Memory Management</font>

---
- The Windows virtual memory manager controls how memory is allocated and how paging is performed
- Platforms vary from 4kB to 64lB
- Intel and AMD64 have 4kB per page
- Intel Itanium platforms have 8kB per page


# Windows Virtual Address Map
- on 32-bit platforms, each Windows users process sees a separate 32-bit address space, allowing 4GB of VM per process
- Half of the memory is reserved for the OS
- Most of the upper 2GB is in kernel mode
- Large memory applications, on both clients and servers, can run more effectively on 64-bits Windows

4 Regions of a 32-Bit User Process
1) `0x0000 0000 to 0x0000 FFFF`
	1) Set aside to help programmers catch NULL-pointer assignments
2) `0x0001 0000 to 0x7FFE FFFF``
	1) Available user address space
	2) Divided into pages that may be loaded into main memory
3) `0x7FFE 0000 to 0x7FFF FFFF``
	2) A guard page inaccessible to the user
	3) Easier for OS to check out-of-bounds pointer references
4) `0x8000 0000 to 0xFFFF FFFF``
	3) System address space
	4) Windows Executive
	5) Kernel
	6) HAL
	7) Device drivers


# Windows Paging
- When a process is created, it can makes use of the entire user space of almost 2GB
- This space is divided into fixed-size pages, any of which can be brought to main memory
- OS manages the addresses in contiguous regions allocated on 64-kB boundariesC

3 Region States
1. Available
	1. Address not currently used by this process
2. Reserved
	1. Addresses that the VM has set aside for a process and cannot be allocated to another
3. Committed
	1. Addresses that the VM has initialized for use by the process
	2. Reside on disk on in physical memory
		1. Files or occupy space in the paging files

![[Pasted image 20250408140125.png|300]]

- Reason for reserved and committed memory
	- Reduced amount of total VM space needed
	- Allows program to reserve addresses without making them accessible to the program or having them charged against their resource quotas


# Windows Swapping
- Paging will hold items that haven't been accessed in a long time, whereas swapping holds items that were recently taken out of memory


# <font style="color:#96DED1"> 8.6 - Android Memory Management</font>

---

- Android includes a number of extensions to the normal Linux kernel memory management facility
	- ASHMem
		- Provides anonymous shared memory
	- ION
		- A memory pool manager that enables its clients to share buffers
	- Low memory killer
		- Most mobile devices do not have a swap capability (flash memory lifetime consideration)
		- When main memory is exhausted, the application using the most memory is either backed off or terminated


# <font style="color:#96DED1"> 8.7 - Summary</font>

---
- To use the processor and I/O facilities efficiently, it is desirable to maintain as many processes in main memory as possible
- Also to free programmers from size restrictions in program development
- With virtual memory, all addresses references are logical referenced translated at run time to read addresses
- VM also allows processes to be broken up into prices, that are not contiguous
- Two approaches to provide VM are paging and segmentation
- Paging, each process is divided into small, fixed size pages
- Segmentation provides pieces of varying size
- VM requires both hardware and software
	- Dynamic translation of virtual addresses to physical
	- Generation of an interrupt when a referenced page or segment is not in main memory
- Design issues
	- Fetch policy
	- Placement policy
	- Replacement policy
	- Resident set management
	- Cleaning policy
	- Load control



# <font style="color:#96DED1"> 8.8 - Key Terms, Review Questions, and Problems</font>

---

