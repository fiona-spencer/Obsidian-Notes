- Range and variety of computer systems that operating systems are designed for
	- Embedded systems
	- Smart phones
	- Single-user workstations
	- Personal computers
	- Share systems
	- Large mainframe systems
	- Supercomputers
	- Real-time systems

Example Systems
- Windows
	- Multitasking OS for personal computers, workstations, servers, and mobile devices
	- One of the first commercial OS that uses object-oriented design principles
- Android
	- Embedded devices, especially mobile phones
- UNIX
	- Multiuser OS, for minicomputers
	- FreeBSD is widely used system that use new features
	- Solaris is a commercial version in UNIX
- Linux
	- An open-source version of UNIX

# <font style="color:#96DED1"> 1.1 - Basic Elements</font>

---

- An operating system (OS) exploits the hardware resources of one or more processors to provide a set of services to system users
- OS manages secondary memory and I/O
- Top level of a computer:
	- Processor (CPU)
	- Memory
	- I/O modules
	- System bus
- The interconnected system works to execute programs
- For the processor to exchange data with memory 2 internal registers are used:
	- Memory address register (MAR)
		- Address in memory for the next read or write
	- Memory buffer register (MBR)
		- Contains the data to be writing into memory
	- I/O address register (I/OAR)
		- Specifies the I/O device
	- I/O buffer register (I/OBR)
		- Exchange data between I/O module and processor
- A memory module consists of a set of locations, defined by sequentially numbered addresses
- Each location contains bit patterns (machine code) that is an instruction or data
- An I/O module transfers data from external devices to processors and memory, and vice versa
- It also contains internal buffers for temporarily storing data until they can be sent on

![[Pasted image 20250318155742.png|500]]


# <font style="color:#96DED1"> 1.2 - Evolution of Microprocessor</font>

---

- Handheld computing was enabled by microprocessors
	- Sub-nanosecond timeframes
- Each chip, called a socket, contains multiple processors (cores)
- Each with multiple levels of large memory caches, and multiple logical processors sharing the execution units of each core
- In 2010, laptops can have 2 to 4 cores, that have 2 hardware threads, for a total of 4 or 8 logical processors
- Graphical Processing Units (GPUs) provide computation on arrays of data using Single-Instruction Multiple Data (SIMD) techniques
- CPUs have powerful array operations from vector units integrated into the processor architecture of the x86 and AMD64 families
- Digital Signal Processors (DSPs) help streaming audio and video
- Other specialized computational devices (fixed function units) co-exist with the CPU to support other computations
- A microprocessor is given way to the System on a Chip (SoC), where it contains most of the components, such as DSPs, CPUs, I/O devices, and main memory


# <font style="color:#96DED1"> 1.3 - Instruction Execution </font>

---

- A program that is executed by a processor consists of a set of instruction stored in memory
- Instruction Cycle:
	- Fetch stage
	- Execute stage

![[Pasted image 20250318160601.png|500]]

- The processing required for a single instruction is called an _instruction cycle_
- The programs halts when the processor is turned off, or encounters a halt instruction
- The program counter (PC) holds the address of the next address instruction to be fetched
- The program counter increments after each fetch instruction
- The fetched instruction is loaded into the instruction register (IR)
- The processor performs the required actions
	- Processor-memory
	- Processor-I/O
	- Data processing
		- Arithmetic or logical operation
	- Control
- The processor contains a single data register called the accumulator (AC)

![[Pasted image 20250318161116.png|400]]

- Both instructions and data are 16-bits
- Memory is organized as a sequences of 16-bit words
- Instruction format
	- 4 bits/1 hex for each opcode
	- $2^4 = 16$ different opcodes
	- 12-bits/3 hex for the instruction format
	- $2^{12}$ = 4096 (4K) words of memory can be directly addressed

Program Execution Example
![[Pasted image 20250318161456.png|400]]

1. PC contains 300, moves 1940 to IR, and the PC is incremented
	1. Uses MAR and MBR (intermediate registers)
2. The 1xxx in 1940 indicates that the AC is to be loaded from memory, from 940 address
	1. Therefore, 3 goes to AC
3. The next instruction 5941 is fetches, PC is incremented
4. 5xxx indicates add AC from memory, where 941 is the address
	1. 2 is added to AC
5. The next instructions is 2941 is fetched, and PC is incremented
6. 2xxx indicates store AC to memory, where address is 941

- Three instruction cycles, each consisting of a fetch stage and an execute stage, are used to add the contents of location 940 to the contents of 941
- Most modern instructions contain more than one address for memory reference


# <font style="color:#96DED1"> 1.4 - Interrupts</font>

---

- Computers provide mechanisms where modules (I/O, memory) may interrupt the normal sequencing of the processor

Class of Interrupts
- Program
- Timer
- I/O
- Hardware failure

- Interrupts improves processor utilization
![[Pasted image 20250318162510.png]]

- Solid lines represents segments of code in a program
- Dashed lines represents the path of execution of the processor
- (4) is the I/O operation
- (5) is the sequence of instruction to complete the operation

# Interrupts and the Instruction Cycle

- With interrupts, the processor can be engaged in executing other instructions while an I/O operation is in progress
- In the short I/O wait interrupt, the I/O modules send an _interrupt request_ signal to the processor
- The processor suspends the current program, and branches of to the I/O devices
- When interrupt hander is finished, the processor resumes the original execution
- Interrupts can occur at any point in the main program
- For user program, an interrupt suspends the normal sequence of execution
- The processor and OS are responsible for suspending the user program, and resuming it
- An _interrupt stage_ is added to the instruction cycle

![[Pasted image 20250318163403.png|600]]

- The interrupt-hander routine is part of the OS
- Overhead is needed to implement interrupts, although is more efficient than to wait for I/O operation

# Interrupt Processing

![[Pasted image 20250318163830.png|400]]

- The interrupt triggers a series of multiple event in the processor hardware and software

When an I/O devices is completed, the follow hardware events occur:
1) The devices issues an interrupt signal to the processor
2) The processor finishes execution of the current instruction before responding
3) The processor test for a pending interrupt request, and sends an acknowledgment signal to the corresponding device
	1) This removes the interrupt signal from the device
4) The process prepares to transfer control to the interrupt routine
	1) Saves information needed to resume the current program at the point of interrupt
		1) Program status word (PSW)
		2) Location of the next instruction (in the PC)
	2) These are pushed onto a control stack
5) Process loads the program counter with the entry location of the interrupt-handing routine
	1) If there is more than one interrupt routine, processor determines by issuing request to the devices, or original interrupt specification is included
6) When PC is loaded, the processor fetches the instruction cycle
	1) PSW relating to the interrupted program is saved on the control stack
	2) Contents of the processor register, and other memory is saved
	3) A user program is interrupted after the instruction at _N_
	4) The contents of all the registers and the address of the next instruction _(N+1)_, a total of M words, are pushed on to the control stack
	5) The stack pointer is updated to the new top of stack, and program counter is updated to the beginning of the interrupt service routine
7) Interrupt examines status information
8) Interrupt processing is complete, the saved register values are retrieved from the stack and restores in the register (figure 1.11 b)
9) Restore PSW and program counter values from the stack
	1) The next instruction will be from the interrupted program




![[Pasted image 20250318164021.png|600]]

# Multiple Interrupts
- Multiple interrupts can be handled in 2 ways:
	- Sequential interrupt processing
	- Nested interrupt processing

![[Pasted image 20250318165425.png|400]]

- In strict sequential order, _disabled interrupts_ are used to put interrupts in a pending state if a interrupt is currently being processed
- Although this does not take into account relative priority or time-critical needs
- The second approach defines priorities for interrupts and allows an interrupt of higher priority
- User information is placed on the control stack and corresponding interrupt service routine (ISR)

![[Pasted image 20250318165901.png|450]]

# <font style="color:#96DED1"> 1.5 - The Memory Hierarchy </font>

---

- 3 constraints of memory
	- Capacity
	- Access time
	- Cost
- Faster access time, greater cost per bit
- Greater capacity, smaller cost per bit
- Greater capacity, slower access speed

- Memory hierarchy is the organization of different types of memory based on speed, cost, and capacity
- Going down the hierarchy, the following occurs
	- Decreasing cost per bit
	- Increasing capacity
	- Increasing access time
	- Decreasing frequency of access to the memory by the processor

![[Pasted image 20250318183220.png|400]]

Level 1 => 1000 bytes and access time of 0.1 µs
Level 2 => 100,000 bytes and access time of 1 µs

![[Pasted image 20250318183236.png|400]]

- The average access time to a two-level memory as a function of the **hit ratio** *H*, where *H* is defined as the fraction of all memory accesses that are found in the faster memory (cache), $T_1$ is the access time to level 1, and $T_2$ is the access time to level 2
- For high percentages of level 1 access, the average total access time is much closer to that of level 1 than that of level 2

Ex.
- Support 95% of the memory accesses are found in the cache (H = 0.95)
- The average time to access a byte is

$(0.95)(0.1µs)+(0.05)(0.1µs+1µs)=0.095+0.055=0.15µs$

- The principle of known as locality of reference is a key concept in computer memory management that describes how programs access memory locations in a predictable manner
- Memory references by the processor, for instructions and data, tend to cluster
- Loops and subroutines are repeated references
- Similarly, operations on tables and arrays involve access to a cluster set of data bytes
- Data can be organized across the hierarchy so the percentage of accesses to each successively lower level is less than the above level
- The fastest, smallest, and most expensive type of memory consists of the registers internal to the processor; processor registers
- Cache is inside on near the CPU used for CPU operations and reduces delays
- Main memory (RAM) is on the motherboard and used to handle large programs and multitasking
- Registers, cache, and RAM are volatile and use semi-conductor technology
- Data is stores on permanent external storage devices; secondary memory
	- Hard disk
	- Removable disk
	- Tape
	- Optical storage
- Hard disk is also used to provide an extension to main memory known as virtual memory
- A portion of main memory can be used as a buffer to temporarily hold data that are to be read out to disk; disk cache
	- Disk writes are clustered
	- Referenced data is closer to the cache

# <font style="color:#96DED1"> 1.6 - Cache Memory </font>

---

- Cache interacts with other memory management hardware
- Principles used in virtual memory schemes are also applied in cache memory

# Motivation

- In all instruction cycles, the processor accesses memory at least once
- The rate of the processor is limited by the memory cycle time
- Memory cycle time has improved and should be on par with processor cycle times
- This is expensive, so we can exploit the principle of locality by providing a small, fast memory between the processor and main memory, called cache

# Cache Principles

![[Pasted image 20250318185439.png|400]]

- Cache memory is intended to provide memory access time approaching that of the fastest memory available
- The cache contains a copy of a portion of main memory
- A block of main memory is read into the cache then the byte or word is delivered to the processor
- The higher the level the slower memory cycle times and larger the size of memory
- Main memory consists of up to $2^n$ addressable words, with each word a unique $n$-bit address
- For mapping purpose, this memory consists of a number of fixed length blocks of _K_ words each
- There are $M = 2^n/K$ blocks
- Cache consists of C slots/lines of _K_ words each
- Where slots are less than the number of main memory blocks (C << M)
- Each slot includes a tag that identifies which particular block is currently being stores
- The tag is a number of higher-order bits of the address, and refers to all addresses that begin with that sequences of bits

![[Pasted image 20250318185741.png|500]]

- 6-bit address and 2-bit tag
- The tag refers to the block of locations
- The processor generates the address, RA, of a word to be read
- If it is contained in the cache, it is delivered to the processor
- Otherwise, the block containing that word is loaded into the cache

![[Pasted image 20250318190055.png|300]]

# Cache Design

Memory and Disk Cache Design:
- Cache size
- Block size
- Mapping function
- Replacement algorithm
- Write policy
- Number of cache levels

- Small caches can have a significant impact on performance
- Block size must be in equilibrium for maximum efficiency
- When a new block of data is read into cache, the mapping function determines which cache location the block will occupy
	- A more complex mapping function will increase the hit ratio, but also complicate circuitry
- Replacement algorithm chooses which block to replace when a new block is loaded into cache
	- Use least-recently-used (LRU) algorithm
	- Hardware mechanisms are needed to use LRU
- When the block is altered, the write policy dictates when the memory write operation takes place
	- Every time block is updated or when block is replaced
- Multilevel caches are used for better efficiency

# <font style="color:#96DED1"> 1.7 - Direct Memory</font>

---

3 techniques for I/O operations
1) Programmed I.O
2) Interrupt-driven I/O
3) Direct memory access (DMA)

Programmed I/O
- In programmed I/O, the I/O module performed the requested action, then sets the appropriate bits in the I/O status register
- After the I/O instruction is invoked, the processor determines when the I/O instruction is completed
- Processor periodically checks the status of the I/O module
- Process waits a long time

Interrupt-Driven I/O
- Processor issues an I/O command to the module, then do other work
- I/O when then interrupt the processor to request service when ready
- Processor executes data transfer, and resumes its former processing
- More efficient than programmed I/O

Drawbacks of Programmed and Interrupt-Driven I/O
1. I/O transfer rate is limited by the speed that the processor can test and service a device
2. The processor is tied up in managing an I/O transfer

Direct Memory Access (DMA)
- Use for moving large volumes of data
- Performed by a separate module on the system bus, or incorporated into an I/O module
- When processor wishes to read/write a block of data, it issues a command to the DMA module by sending the following information:
	- Read/write request
	- Address of the I/O device involved
	- Starting location in memory
	- Number of words to read/write
- The DMA module transfers the entire block of data without processor
- When transfer is complete, DMA module sends an interrupt signal to processor
- Bus competition
	- Slowed processor if is also needs to use the bus
- For multi-word I/O transfer, DMA is more efficient

# <font style="color:#96DED1"> 1.8 - Multiprocessor and Multicore Organization</font>

---

- Computer has been viewed as a sequential machine
- At the micro-operation level, multiple control signals are generated at the same time
- Pipelining, and overlapping are examples of functions in parallel
- Parallelism has improved performance and reliability

3 Parallel Processors
- Symmetric multiprocessors (SMPs)
- Multicore computers
- Clusters

# Symmetric Multiprocessors
- An SMP is a stand-alone computer system with:
	- Two or more similar processors of comparable capability
	- Processors share the same main memory and I/O
	- All processors share access to I/O
	- All processors can perform the same function
	- The system is controlled by an integrated OS

Advantages of SMP
- Performance
	- Work can be done in parallel
- Availability
	- Failure of a processor does not halt the machine
- Incremental growth
	- Add additional processors
- Scaling
	- Range of configurations

- The OS takes care of scheduling of tasks on individual processors, and synchronization among processors

*ORGANIZATION*

![[Pasted image 20250318192559.png|400]]

- Multiple processors, each with its own control unit, ALU, and registers
- Each processor has two levels of cache, and access to shared main memory and I/O
- Processors communicate through memory (messages and status information in shared address spaces)
- Processors may also have at least one level of cache private to the processor
	- All processors must be altered when the private cache is changed; coherence problem

# Multicore Computers
- A multicore computer, also known as a chip multiprocessor, combines two or more processors (cores) on a single piece of silicon (die)
- Each core consist of all the components of an independent processor
- To improve performance, designers place many processors and cache memory on a single chip
	- The Intel Core i7-5960X has 6 x86 processors, each with a L2 cache, and shared L3 cache
	- Prefetching, examines memory access patterns and attempts to fill cache speculatively with data that is likely to be requested
- Core i7-5960X chip uses two external communications to other chips
	- DDR4 memory controller
		- Four 8 byte channels, total bus width of 256 bits, aggregate data rate of 64 GB/s
	- PCI express
		- 8 GT/s (transfer per second)


![[Pasted image 20250318193344.png|400]]


# <font style="color:#96DED1"> 1.9 - Key terms, Review Questions, and Problems</font>

---


# Locality
# Operating of Two-Level Memory

# Performance